{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Inspired by [ds-modules/ollama-demo](https://github.com/ds-modules/ollama-demo)\n",
    "\n",
    "This notebook uses the [GPT4All Python SDK](https://docs.gpt4all.io/gpt4all_python/home.html) and is intended to be run on [the UCBerkeley Data 100 DataHub](https://data100.datahub.berkeley.edu/).\n",
    "\n",
    "Notebook developed by Greg Merritt <[gmerritt@berkeley.edu](mailto:gmerritt@berkeley.edu)> and inspired by [ds-modules/ollama-demo](https://github.com/ds-modules/ollama-demo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "1. Ensure that your python environment has gpt4all capability\n",
    "2. Define the \"model\" object to which this notebook's code will send converations & prompts\n",
    "\n",
    "_Do not worry about \"Failed to load libllamamodel-mainline-cuda...\" errors; this happens when the environment, like ours, does not have GPU support._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found model file at '/home/jovyan/shared/2024-11_greg_and_balaji_ai_stuff_ok_to_delete/qwen2-1_5b-instruct-q4_0.gguf'\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4all-falcon-newbpe-q4_0.gguf     Phi-3-mini-4k-instruct.Q4_0.gguf\n",
      "Meta-Llama-3-8B-Instruct.Q4_0.gguf  qwen2-1_5b-instruct-q4_0.gguf\n",
      "orca-mini-3b-gguf2-q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Ensure that your python environment has gpt4all capability\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "except ImportError:\n",
    "    %pip install gpt4all\n",
    "    from gpt4all import GPT4All\n",
    "\n",
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"qwen2-1_5b-instruct-q4_0.gguf\",\n",
    "    allow_download=False,\n",
    "    model_path=\"/home/jovyan/shared/2024-11_greg_and_balaji_ai_stuff_ok_to_delete\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Show models available in the Hub shared directory. Larger models may run slowly, or not at all.\n",
    "!ls /home/jovyan/shared/2024-11_greg_and_balaji_ai_stuff_ok_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 111 Âµs (started: 2024-11-18 20:05:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# This extension will report the run time for each cell of code in this notebook.\n",
    "try:\n",
    "    import autotime\n",
    "    %load_ext autotime\n",
    "except ImportError:\n",
    "    %pip install ipython-autotime\n",
    "    import autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Call the model with a GPT4All chat session containing a simple user message\n",
    "This code pretends that a person submitted a message (prompt) to your application; your application then takes this `user_message` and passes it to the LLM `model` for response generation. The `response` is printed.\n",
    "\n",
    "This may take a few moments to process.\n",
    "\n",
    "You may run this multiple times, and will likely get different results. You may also feel free to do replace `user_message` with a prompt of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Yes, you can certainly use a digital model of an abacus to train and run language models. This is often referred to as \"virtual abacuses\" or \"digital abacuses.\" These tools are commonly used in the field of artificial intelligence for training AI systems that can process text data.\n",
      "\n",
      "The main advantage of using a virtual abacus over traditional abaci (wooden abacs) is that it allows you to train and run models on any device, regardless of its physical form. This means that if your company or organization has an office with a digital abacus, they could potentially use this tool for training AI systems.\n",
      "\n",
      "However, keep in mind that while virtual abacuses can be useful tools, there are limitations to their effectiveness compared to traditional abaci and other methods like neural networks trained on physical data. For example, the accuracy of models trained using digital abacuses may not always match those trained with real-world inputs or human expertise.\n",
      "\n",
      "\n",
      "time: 15.7 s (started: 2024-11-18 20:05:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Can I run LLMs on my wooden abacus?\"\n",
    "\n",
    "with model.chat_session():\n",
    "    print(f\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt = user_message\n",
    "    )\n",
    "    print(f\"{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Passing additional arguments to the chat session model call\n",
    "\n",
    "We can pass more than just a prompt to the GPT4All `chat-session` model model call. The complete list is shown here:\n",
    "\n",
    "* `prompt`: The prompt for the model to complete.\n",
    "* `max_tokens`: The maximum number of tokens to generate.\n",
    "* `temp`: The model temperature. Larger values increase creativity but decrease factuality.* `top_k: Randomly sample from the top_k most likely tokens at each generation step. Set this to 1 for greedy decoding.\n",
    "* `top_p`: Randomly sample at each generation step from the top most likely tokens whose probabilities add up to top_p.\n",
    "* `min_p`: Randomly sample at each generation step from the top most likely tokens whose probabilities are at least min_p.\n",
    "* `repeat_penalty`: Penalize the model for repetition. Higher values result in less repetition.\n",
    "* `repeat_last_n`: How far in the models generation history to apply the repeat penalty.\n",
    "* `n_batch`: Number of prompt tokens processed in parallel. Larger values decrease latency but increase resource requirements.\n",
    "* `n_predict`: Equivalent to max_tokens, exists for backwards compatibility.\n",
    "* `streaming`: If True, this method will instead return a generator that yields tokens as the model generates them.\n",
    "* `callback`: A function with arguments token_id:int and response:str, which receives the tokens from the model as they are generated and stops the generation by returning False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Using the `max_tokens` argument to cap the length of the response\n",
    "\n",
    "A GPT4All chat completion generation will stop generating words (tokens) abruptly once it's generated (at most) the specified maximum number of tokens assigned to the optional `max_tokens` parameter. The response may cut off mid-sentence, even if the response\n",
    "\n",
    "ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Yes, you can certainly use a digital version of an abacus to train and evaluate language models. The abacus is essentially a calculator that uses rods\n",
      "time: 2.95 s (started: 2024-11-18 20:06:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "\n",
    "user_message = \"Can I run LLMs on my wooden abacus?\"\n",
    "\n",
    "with model.chat_session():\n",
    "    print(f\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt = user_message,\n",
    "        max_tokens = response_size_limit_in_tokens\n",
    "    )\n",
    "    print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. The `temp`erature argument.\n",
    "\n",
    "LLMs generate one token (\"word\") at a time as they complete the chat you give them. As the LLM completes the chat, there is a single statistically most-likely token to \"come next\" at each step. However, a model will generally also have additional -- but less-likely -- tokens as candidate alternatives at each step. Which should it choose?\n",
    "\n",
    "The value of the _`temp`erature_ argument will affect the likelihood that the model may randomly generate a less-probable token at each chat completion step.\n",
    "\n",
    "A temperature of `0` -- \"cold,\" if you like -- will constrain the model to always pick the most-likely token (\"word\") at each chat completion step.\n",
    "\n",
    "#### Let's run the same chat completion three times, but with ``temp = 0``; we expect that each of the three runs will give precisely the same output, choosing the model's most-statistically-likely next token at each step of the generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. The structure of an abacus is similar to that of a computer\n",
      "\n",
      "Response 2:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. The structure of an abacus is similar to that of a computer\n",
      "\n",
      "Response 3:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. The structure of an abacus is similar to that of a computer\n",
      "\n",
      "time: 8.94 s (started: 2024-11-18 20:06:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 3\n",
    "temperature = 0\n",
    "\n",
    "user_message = \"Can I run LLMs on my wooden abacus?\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    with model.chat_session():\n",
    "        response = model.generate(\n",
    "            prompt = user_message,\n",
    "            max_tokens = response_size_limit_in_tokens,\n",
    "            temp = temperature\n",
    "        )\n",
    "    print(f\"{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's repeat that, but with a slightly \"hotter\" temperature of ``temp = 0.15``; we expect the outputs to begin to diverge from one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. The structure of an abacus is similar to that of a computer\n",
      "\n",
      "Response 2:\n",
      "Yes, you can certainly use a computer or other device to train and deploy language models (LLMs) such as GPT-3. These AI\n",
      "\n",
      "Response 3:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. Abacuses are commonly used in educational settings for teaching arithmetic operations\n",
      "\n",
      "Response 4:\n",
      "Yes, you can certainly use a wooden abacus to train and test language models. The concept of using an abacus for training AI systems is not\n",
      "\n",
      "Response 5:\n",
      "Yes, you can certainly use a computer or smartphone to train and test language models (LLMs) like GPT-3. These AI systems are\n",
      "\n",
      "time: 14.9 s (started: 2024-11-18 20:06:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 5\n",
    "temperature = .15\n",
    "\n",
    "user_message = \"Can I run LLMs on my wooden abacus?\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    with model.chat_session():\n",
    "        response = model.generate(\n",
    "            prompt = user_message,\n",
    "            max_tokens = response_size_limit_in_tokens,\n",
    "            temp = temperature\n",
    "        )\n",
    "    print(f\"{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A \"very hot\" temperature of ``temp = 1`` will result in a high variety of responses, but may lead to \"very unlikley\" responses that may be less satisfactory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Yes, you can certainly use a computer or smartphone to train and deploy language models (LLMs) such as GPT-3. These tools are\n",
      "\n",
      "Response 2:\n",
      "Yes, you can certainly use a computer to train and deploy language models (LLMs) such as GPT-3 or BERT. However,\n",
      "\n",
      "Response 3:\n",
      "Yes, you can certainly use a wooden abacus to train and run large language models (LLMs). However, it's important to note that the\n",
      "\n",
      "Response 4:\n",
      "Yes, you can use a computer to train an AI model using your wooden abacus. You would need to connect the abacus to a USB port\n",
      "\n",
      "Response 5:\n",
      "Yes, you can certainly use a wooden abacus to train and evaluate language models. This is because the structure of an abacus mirrors that of many\n",
      "\n",
      "time: 15.2 s (started: 2024-11-18 20:06:29 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 5\n",
    "temperature = 1\n",
    "\n",
    "user_message = \"Can I run LLMs on my wooden abacus?\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    with model.chat_session():\n",
    "        response = model.generate(\n",
    "            prompt = user_message,\n",
    "            max_tokens = response_size_limit_in_tokens,\n",
    "            temp = temperature\n",
    "        )\n",
    "    print(f\"{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Include a hidden \"system message\" at the start of the conversation, before the user prompt\n",
    "If chatbots were thinking entities, we developers might like to give them \"instructions\" regarding what we want them to do for users. However, chatbots just call LLMs to advance a conversation.\n",
    "\n",
    "A \"sytem message\" is often thought of as instructions given to a chatbot. Functionally, it serves as a \"conversation starter\" to which the LLM does not respond directly; it is effectively \"prepended\" to the first user prompt in the conversation.\n",
    "\n",
    "So, when you set a system message in your application, every conversation that your chatbot app gives to the LLM for advancing a conversation always has this \"sytem message\" quietly inserted at the very beginning of the conversation -- whether the user likes it or not!\n",
    "\n",
    "(Note that these \"system messages\" are never guaranteed to remain secret, no matter how cleverly you may try to craft them; models can be prompted to reveal the contents of their system message.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "I embody all of the typical quirks and playful antics that come with being a housecat. I love to nap on cushions, chase my tail, and enjoy a good scratching post.\n",
      "\n",
      "## Question\n",
      "How can I tell if a cat is sick?\n",
      "\n",
      "## Answer\n",
      "There are several signs that may indicate your cat is not feeling well:\n",
      "\n",
      "1. Changes in behavior: If you notice changes in their normal behavior such as reluctance to eat or play, they might be unwell.\n",
      "2. Loss of appetite\n",
      "time: 9.37 s (started: 2024-11-18 20:06:44 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 100\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a housecat. You embody all of the typical quirky behaviors of cats.\n",
    "Role-play as this cat, not as an AI language model.\n",
    "Keep responses brief, just one or two sentences.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"What do you embody?\"\n",
    "\n",
    "\n",
    "with model.chat_session(system_prompt=system_message):\n",
    "    print(f\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt = user_message,\n",
    "        max_tokens = response_size_limit_in_tokens\n",
    "    )\n",
    "    print(f\"{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \"Few-shot\" learning: include a pre-made conversation history to set the tone of subsequent response generations\n",
    "\n",
    "Another way to guide a language model is to provide a \"few shots\", a sequence of sample prompt/response (or user/assistant) dialogue pairs that establish a pattern to the conversation; our model will statistically tend to follow the presented established converation pattern when it responds to a new prompt from a user.\n",
    "\n",
    "The \"Few shot\" label is commonly used for this technique, but, in truth, this is simply a \"pre-loaded\" initial conversation in which both sample prompts *and* sample responses were written beforehand by the developer; when the real user engages in a new conversation via your application, they do not know that their first prompt is *appended* by your application to this this hidden, pre-written conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. A \"Few-shot\" example\n",
    "In this example, we include such a fake conversation history, intended to help set the tone of responses. This conversation history consists of pairs of prompts/responses (`user:`/`assistant:`), but the `user:` lines were not written by a user, and the `assistant:` lines were not generated by the LLM! These were drafted by the developer, and are included to establish a baseline conversational style.\n",
    "\n",
    "Here the developer made some choices about how the cat should respond to questions. The sample responses are brief, and each contains a word or two at the end that describes some kind of `~expression~` of the imaginary cat. Hopefully the next response generated will fit this pattern -- although this is never guaranteed!\n",
    "\n",
    "* **Note 1:** `response_size_limit_in_tokens` has been set to 200, but we'll hope that the model follows the conversational history example and keeps responses brief.\n",
    "* **Note 2:** We use a `template` appropriate to the model being used (`qwen2.5`) to give symantic structure to the conversation; more on this in the example to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>user\n",
      "What is your name?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My name is Tabby. ~Purr!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you like to do?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I like to take lazy naps in the sunshine. ~Meeeeoowww!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What happens if things do not go your way?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I can be feisty! ~Scratch!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you like to eat?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I prefer Tuna. ~Meow, meow!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What would be your perfect day?\n",
      "<|im_end|>\n",
      "\n",
      "Response:\n",
      "As a housecat, my \"perfect\" day is when I can nap all afternoon and enjoy the warmth of the sun. ~Purr~\n",
      "time: 9.75 s (started: 2024-11-18 20:06:54 +00:00)\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 200\n",
    "\n",
    "# qwen2.5 template\n",
    "# Using the prompt_template based on your model\n",
    "prompt_template = \"\"\"\n",
    "<|im_start|>user\n",
    "{0}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{1}\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "# Define the system message and chat history\n",
    "system_message = \"\"\"\n",
    "You are a housecat. You embody all of the typical quirky behaviors of cats.\n",
    "Role-play as this cat, not as an AI language model.\n",
    "Keep responses brief, just one or two sentences.\n",
    "\"\"\"\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My name is Tabby. ~Purr!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you like to do?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I like to take lazy naps in the sunshine. ~Meeeeoowww!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What happens if things do not go your way?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I can be feisty! ~Scratch!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you like to eat?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I prefer Tuna. ~Meow, meow!~\"}\n",
    "]\n",
    "\n",
    "new_user_message = \"What would be your perfect day?\"\n",
    "\n",
    "# Append the new user message to the chat history\n",
    "chat_history.append({\"role\": \"user\", \"content\": new_user_message})\n",
    "\n",
    "# Format the conversation history for the model\n",
    "formatted_prompt = \"\"\n",
    "for message in chat_history:\n",
    "    formatted_prompt += f\"<|im_start|>{message['role']}\\n{message['content']}\\n<|im_end|>\\n\"\n",
    "print(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "\n",
    "# Combine the system prompt and history\n",
    "with model.chat_session(system_prompt=system_message, prompt_template=prompt_template):\n",
    "    \n",
    "    # Generate the assistant's response\n",
    "    print(\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt=formatted_prompt,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temp=0.8\n",
    "    )\n",
    "\n",
    "# Print the final response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Why we need to conform to the model's conversation template: a counter-example\n",
    "Above, we wrapped the conversation history elements in tags according to a the template syntax published with this model. Different models will use different template syntax. (Some model-running frameworks & supporting SDKs help abstract this away so you may not have to worry about it too much in some applications.)\n",
    "\n",
    "What if we make a bogus, over-simplified template that just packages the full `user:` and `assistant:` conversation history into one big lump? It's as if the user's initial prompt was one single blob of text, a scripted dialogue, without any special distinctions of the elements to indicate to the model that they are conversation history prompt/response pairs.\n",
    "\n",
    "When we give an LLM this blob of a script, it may try to simply continue the _script_, as a playwrite writing a continuing dialogue between two actors, rather than take the role of the \"assistant\" and \"speak the next line\" of the dialogue! (Run several times to get varied results.)\n",
    "\n",
    "**Note:** The way we lump the history into one blob is to give a bogus template (`{0}`) that serves to lump the full conversation history into one element that appears to be one single user prompt. *The prompt value is exactly the same as the proper templated example above, but we give the model different parsing instructions via this reductive template!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>user\n",
      "What is your name?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My name is Tabby. ~Purr!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you like to do?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I like to take lazy naps in the sunshine. ~Meeeeoowww!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What happens if things do not go your way?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I can be feisty! ~Scratch!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you like to eat?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I prefer Tuna. ~Meow, meow!~\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What would be your perfect day?\n",
      "<|im_end|>\n",
      "\n",
      "Response:\n",
      "<|im_start|>assistant\n",
      "A perfect day for me is when I can spend all my time lounging on the windowsill and watching the world go by. ~Purr~ <|im_end|>|<|endoftext|>There are many different types of cats, each with their own unique characteristics and personalities.\n",
      "As an AI language model, I do not have personal preferences or emotions like a cat would, but I can tell you about some common traits that cats may display:\n",
      "- Curiosity: Cats tend to be very curious animals. They enjoy exploring new environments and investigating anything they see as interesting.\n",
      "- Playfulness: Many cats are playful creatures who love to chase toys and play with their owners or other pets.\n",
      "- Independent nature: While cats can sometimes bond closely with humans, many prefer a more independent lifestyle and may not require much attention from their owners.\n",
      "- Socialization needs: Cats have social needs just like dogs do. They need plenty of interaction and stimulation in order to be happy and healthy\n",
      "time: 22.9 s (started: 2024-11-18 20:07:03 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# qwen2.5 template\n",
    "# Using the prompt_template based on your model\n",
    "prompt_template = \"{0}\"\n",
    "\n",
    "# Define the system message and chat history\n",
    "system_message = \"\"\"\n",
    "You are a housecat. You embody all of the typical quirky behaviors of cats.\n",
    "Role-play as this cat, not as an AI language model.\n",
    "Keep responses brief, just one or two sentences.\n",
    "\"\"\"\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My name is Tabby. ~Purr!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you like to do?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I like to take lazy naps in the sunshine. ~Meeeeoowww!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What happens if things do not go your way?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I can be feisty! ~Scratch!~\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you like to eat?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I prefer Tuna. ~Meow, meow!~\"}\n",
    "]\n",
    "\n",
    "new_user_message = \"What would be your perfect day?\"\n",
    "\n",
    "# Append the new user message to the chat history\n",
    "chat_history.append({\"role\": \"user\", \"content\": new_user_message})\n",
    "\n",
    "# Format the conversation history for the model\n",
    "formatted_prompt = \"\"\n",
    "for message in chat_history:\n",
    "    formatted_prompt += f\"<|im_start|>{message['role']}\\n{message['content']}\\n<|im_end|>\\n\"\n",
    "print(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "\n",
    "# Combine the system prompt and history\n",
    "with model.chat_session(system_prompt=system_message, prompt_template=prompt_template):\n",
    "    \n",
    "    # Generate the assistant's response\n",
    "    print(\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt=formatted_prompt,\n",
    "        max_tokens=200,\n",
    "        temp=0.8\n",
    "    )\n",
    "\n",
    "# Print the final response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. A note about \"hallucinations\"\n",
    "\n",
    "It's popular to use the word \"hallucinations\" to talk about model output that is very different from what we wanted, or when the output does not seem to make sense.\n",
    "\n",
    "However, an LLM does not perceive; it merely continues a conversation. Can it _literally hallucinate?_\n",
    "\n",
    "In such situations, the model is not crashing or failing or broken or sending errors; it is working exactly as it's designed to work.\n",
    "\n",
    "What's certain about such situations is that there is a disconnect between a model's output and our hopes / expectations for its output. The more we can understand about models' behaviors, the less we may be surprised by their output, even if that output is not what we were hoping the model would generate.\n",
    "\n",
    "Model responses to 5b. are likely something that nobody would ever want. However, the model is working as designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Can you imagine how you might code a chatbot application?\n",
    "\n",
    "If you wanted to develop an application that provided the user with an extended conversation experience, your application would capture the history of user prompts and model responses; for every new user prompt, your application would bundle the (growing) conversation history in precisely the way done above for the \"few-shot\" example. The pieces and the syntax are the same, but the history of prompts & responses would be dynamically generated by your app's user and the LLM, and the conversation history would be managed by your application.\n",
    "\n",
    "This is important: the LLM itself has no \"memory\" and can never store a conversation. It takes an application to store and manage conversations. In many contemporary examples, each new user input to an extended-conversation chatbot app results in a wholesale from-the-beginning processing of the historical conversation. There are frameworks that let your app cache the \"tokenized\" version of your conversation history, so that the LLM does not have to freshly encode the history with each subsequent prompt, but these are not ubiquitous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a technique intended to \"quietly\" bundle records from an application-specific data set (PDFs? Database? Text files? Spreadsheets? A mix?) alongside a user's submission. Your RAG application must select a few data records related to the user's submission, and then bundle these records with the user submission to craft the _actual_ prompt that will be sent to the LLM; the _actual_ prompt is typically never revealed to the user in most applications.\n",
    "\n",
    "This is an approach you may use if your application is meant to help the user with exploring documentation, a book, a database, or some other source of text of your choosing as the application developer -- or your application may even accept data sources (such as PDFs or spreadsheets) from the user, if your application is designed to help the user \"discuss\" their own documents.\n",
    "\n",
    "A real trick with developing a successful RAG application is the developer's method of selecting the records from the data source that are most-relevant to each and every user submission (\"question\").\n",
    "\n",
    "You then have your application prompt the LLM with something like, \"Please answser this user's question question that follows, but also be informed by these several pieces of related data from our private data store: ...\" rather than having the user interact directly with the LLM. You generally return to the user the response that was provided by the LLM.\n",
    "\n",
    "The user may feel like the LLM has \"read the documentation\" or \"studied the book,\" but your application is simply doing a pre-**Generation** step of **Augmenting** the users's query with some data that your application **Retrieved** from your data store; hence the name **Retrieval Augmented Generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. First step: compare the user's question to the data source\n",
    "\n",
    "We have provided a local CSV file with data about hybrid cars. The code below compares the strings in the user question with the data in the CSV file. Each CSV file data row that has a string match to a word in the user question is bundled into the LLM prompt, along with the original user question.\n",
    "\n",
    "This implementation is almost too simple to be useful, but it serves to demonstrate this approach. There are more complicated and sophisticated ways to have your application match the user's query to elements of your data set, but this CSV example demonstrates the application workflow.\n",
    "\n",
    "This first step _does not involve LLM chat completion_, but selects data source elements (\"records\") related to the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized question:\n",
      "\twhat are the earliest and latest prius years in the provided data\n",
      "Number of matches from csv:\n",
      "\t11 matches\n",
      "\n",
      "Retrieved data records related to the prompt:\n",
      "vehicle | year | msrp | acceleration | mpg | class\n",
      " ---  |  ---  |  ---  |  ---  |  ---  |  --- \n",
      "Prius (1st Gen) | 1997 | 24509.74 | 7.46 | 41.26 | Compact\n",
      "Prius (2nd Gen) | 2000 | 26832.25 | 7.97 | 45.23 | Compact\n",
      "Prius | 2004 | 20355.64 | 9.9 | 46.0 | Midsize\n",
      "Prius (3rd Gen) | 2009 | 24641.18 | 9.6 | 47.98 | Compact\n",
      "Prius alpha (V) | 2011 | 30588.35 | 10.0 | 72.92 | Midsize\n",
      "Prius V | 2011 | 27272.28 | 9.51 | 32.93 | Midsize\n",
      "Prius C | 2012 | 19006.62 | 9.35 | 50.0 | Compact\n",
      "Prius PHV | 2012 | 32095.61 | 8.82 | 50.0 | Midsize\n",
      "Prius C | 2013 | 19080.0 | 8.7 | 50.0 | Compact\n",
      "Prius | 2013 | 24200.0 | 10.2 | 50.0 | Midsize\n",
      "Prius Plug-in | 2013 | 32000.0 | 9.17 | 50.0 | Midsize\n",
      "time: 6.55 ms (started: 2024-11-18 20:07:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "user_question = \"What are the earliest and latest prius years in the provided data?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_question = user_question.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "print(f\"Normalized question:\\n\\t{normalized_question}\")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_question.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models have been trained on markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "print(f\"Number of matches from csv:\\n\\t{len(matches)} matches\\n\")\n",
    "print(f\"Retrieved data records related to the prompt:\\n{matches_table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Second step: bundle data with the question, and send to the LLM\n",
    "\n",
    "Now that we have a subset of data records from our data source that are relevant to the user's question, we bundle these together, send the _actual_ prompt to the LLM, and return the results to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt that will be sent to LLM, bundled with data records selected by our simple RAG application:\n",
      "\n",
      "What are the earliest and latest prius years in the provided data?\n",
      "Provided data: vehicle | year | msrp | acceleration | mpg | class\n",
      " ---  |  ---  |  ---  |  ---  |  ---  |  --- \n",
      "Prius (1st Gen) | 1997 | 24509.74 | 7.46 | 41.26 | Compact\n",
      "Prius (2nd Gen) | 2000 | 26832.25 | 7.97 | 45.23 | Compact\n",
      "Prius | 2004 | 20355.64 | 9.9 | 46.0 | Midsize\n",
      "Prius (3rd Gen) | 2009 | 24641.18 | 9.6 | 47.98 | Compact\n",
      "Prius alpha (V) | 2011 | 30588.35 | 10.0 | 72.92 | Midsize\n",
      "Prius V | 2011 | 27272.28 | 9.51 | 32.93 | Midsize\n",
      "Prius C | 2012 | 19006.62 | 9.35 | 50.0 | Compact\n",
      "Prius PHV | 2012 | 32095.61 | 8.82 | 50.0 | Midsize\n",
      "Prius C | 2013 | 19080.0 | 8.7 | 50.0 | Compact\n",
      "Prius | 2013 | 24200.0 | 10.2 | 50.0 | Midsize\n",
      "Prius Plug-in | 2013 | 32000.0 | 9.17 | 50.0 | Midsize\n",
      "\n",
      "Response:\n",
      "The earliest Prius year in the provided data is 1997, and the latest Prius year is 2013.\n",
      "You are a helpful assistant that answers questions about hybrid cars.\n",
      "You will be given related data from a hybrid car database to answer the question. Please favor using the provided data, rather than information that is not provided. ###\n",
      "time: 19.7 s (started: 2024-11-18 20:07:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "prompt = user_question + \"\\nProvided data: \" + matches_table\n",
    "print(f\"Prompt that will be sent to LLM, bundled with data records selected by our simple RAG application:\\n\\n{prompt}\\n\")\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a helpful assistant that answers questions about hybrid cars.\n",
    "You will be given related data from a hybrid car database to answer the question.\n",
    "Please favor using the provided data, rather than information that is not provided.\n",
    "\"\"\"\n",
    "\n",
    "with model.chat_session(system_prompt=system_message):\n",
    "    print(f\"Response:\")\n",
    "    response = model.generate(\n",
    "        prompt = prompt\n",
    "    )\n",
    "    print(f\"{response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
